1. What is the definition of Hive? What is the present version of Hive?
ans : Hive is a data warehouse system which is used to analyze structured data. It is built on the top of Hadoop. It was developed by Facebook. Hive provides the functionality of reading, writing, and managing large datasets residing in distributed storage.
current version of Hive is 3.2.1

2.Is Hive suitable to be used for OLTP systems? Why?
ans: No, it doesn't support OLTP because it can't do row level insertion and updation.

3.How is HIVE different from RDBMS? Does hive support ACID transactions. If not then give the proper reason.
ans: RDBMS stands for Relational Database Management System. RDBMS is a such type of database management system which is specifically designed for relational databases. RDBMS is a subset of DBMS.
Hive is a data warehouse software system that provides data query and analysis. Hive gives an interface like SQL to query data stored in various databases and file systems that integrate with Hadoop. Hive helps with querying and managing large datasets real fast.
Hive is a data warehouse database where the data is typically loaded from batch processing for analytical purposes and older versions of Hive doesn’t support ACID transactions on tables. Though in newer versions it supports by default ACID transactions are disabled and you need to enable it before start using it.

4.Explain the hive architecture and the different components of a Hive architecture?
ans: Hive uses a distributed system to process and execute queries, and the storage is eventually done on the disk and finally processed using a map-reduce framework. It resolves the optimization problem found under map-reduce and hive perform batch jobs which are clearly explained in the workflow.
User Interface (UI) – 
As the name describes User interface provide an interface between user and hive. It enables user to submit queries and other operations to the system. Hive web UI, Hive command line, and Hive HD Insight (In windows server) are supported by the user interface. 
 
Hive Server – It is referred to as Apache Thrift Server. It accepts the request from different clients and provides it to Hive Driver.
Driver – 
Queries of the user after the interface are received by the driver within the Hive. Concept of session handles is implemented by driver. Execution and Fetching of APIs modelled on JDBC/ODBC interfaces is provided by the user. 
 
Compiler – 
Queries are parses, semantic analysis on the different query blocks and query expression is done by the compiler. Execution plan with the help of the table in the database and partition metadata observed from the metastore are generated by the compiler eventually. 
 
Metastore – 
All the structured data or information of the different tables and partition in the warehouse containing attributes and attributes level information are stored in the metastore. Sequences or de-sequences necessary to read and write data and the corresponding HDFS files where the data is stored. Hive selects corresponding database servers to stock the schema or Metadata of databases, tables, attributes in a table, data types of databases, and HDFS mapping. 
 
Execution Engine – 
Execution of the execution plan made by the compiler is performed in the execution engine. The plan is a DAG of stages. The dependencies within the various stages of the plan is managed by execution engine as well as it executes these stages on the suitable system components.

5. Mention what Hive query processor does? And Mention what are the components of a Hive query processor?
ans: Hive query processor creates a graph out the Sql like query written.
Components:
Parse and Semantic Analysis (ql/parse)
Metadata Layer (ql/metadata)
Type Interfaces (ql/typeinfo)
Sessions (ql/session)
Map/Reduce Execution Engine (ql/exec)
Plan Components (ql/plan)
Hive Function Framework (ql/udf)
Tools (ql/tools)
Optimizer (ql/optimizer)

6. What are the three different modes in which we can operate Hive?
ans: Local mode: In Hive local mode, Map Reduce jobs related to Hive run locally on a user machine. This is the default mode in which Hadoop uses local file system.
Distributed Mode: In this mode, Hive as well as Hadoop is running in a fully distributed mode. NameNode, DataNode, JobTracker, TaskTracker etc run on different machines in this mode.
Pseudo-distributed Mode: This is the mode used by developers to test the code before deploying to production. In this mode, all the daemons run on same virtual machine. With this mode, we can quickly write scripts and test on limited data sets.

7. Features and Limitations of Hive.
ans:
Features:
----------
Scalable
Fast
Can process large datasets
Processed data stored in hdfs and schema stored in DB.
Limitations:
Hive doesn't support OLTP. Hive supports Online Analytical Processing (OLAP), but not Online Transaction Processing (OLTP).
It doesn't support subqueries.
It has a high latency.
Hive tables don't support delete or update operations.

8. How to create a Database in HIVE?
Create database DB;

9. How to create a table in HIVE?
Create table table_name(
ID int,
Name string
) row format delimited
fields terminated by ',';

10.What do you mean by describe and describe extended and describe  formatted with respect to database and table
ans:
Table-
Describe: shows list of columns 
Describe extended: shows list of columns,metadata of table 
Describe formatted: shows list of columns,metadata of table in tabular format

Database-
Describe: shows name of db,comments, root file location 
Describe extended: shows name of db,comments, root file location , dbproperties

11.How to skip header rows from a table in Hive?
ans: tblproperties("skip.header.line.count"="1");

12.What is a hive operator? What are the different types of hive operators?
ans: hive operators are logic building blocks, 
4 types are:Relational,logical,arithmetic and complex.

13.Explain about the Hive Built-In Functions
THey are similar to sql functions:
round(a),double(a),ceil(a),rand(),concat(A,B)

14. Write hive DDL and DML commands.
DDL -
create,alter,describe
DML-
select,insert

15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and 
CLUSTER BY in Hive.
Answer: order by- ascending or descending sorting of data globally in one reducer.
sort by - sorted data per reducer not globally sorted.
distribute by - distribute the input rows among reducers acc to key
clusterf by - distribute by + sort by 
-> data partititoned in reducer  acc. to key
-> data sorted in each reducer data acc. to key

16.Difference between "Internal Table" and "External Table" and Mention  when to choose “Internal Table” and “External Table” in Hive?
ans: Internal Table - Data stored in Hive's data warehouse 
When dropping table both data and metadata are deleted
Default table in Hive
External Table - When you get data from HDFS or any external source
When dropping the table only metdata is deleted
External table is like a pointer to data in external source.

Use external table when you have data in an external source and you do not want data to be drop even after drop table.
Use Internal table when you want Hive to manage data.

17.Where does the data of a Hive table get stored?
ans : Data is stored in Data warehouse directory of Hive and metdata is store in metastore.

18.Is it possible to change the default location of a managed table?
ans: Yes, you can change the location of a managed table by using the LOCATION '' syntax.

19.What is a metastore in Hive? What is the default database provided by  Apache Hive for metastore?
ans: Metastore - saves metdata of table , like the no. of columns, partitions etc.
default database provided by Apache Hive for metastore is derbyDB.

20.Why does Hive not store metadata information in HDFS?
ans: Because Hive stores metadata in RDBMS and not in HDFS as RDBMS read/writes are faster.

21.What is a partition in Hive? And Why do we perform partitioning in Hive?
ans: Dividing the table in parts based on a column.
This is done as the query time is fast as data in storedin slices.

22.What is the difference between dynamic partitioning and static  partitioning?
ans: Dynamic partitioning: Not sure how many unique values in choosen partition column.
Takes long time while loading data.
eg: insert overwrite tablename partition(colname)
Static Partitioning:value of partition column will be known to us.
eg: insert overwrite tablename partition(colname = value)

23.How do you check if a particular partition exists?
ans :Show partitions tablename

24.How can you stop a partition form being queried?
ans: by using ENABLE OFFLINE at the end of alter table clause.

25.Why do we need buckets? How Hive distributes the rows into buckets?
ans: Buckets help in decomposing the table into further manageable parts.
Every bucket is stored as a file inside partition directory or the table directory.
Hive distributes the rows into buckets usinh hash function.

26.In Hive, how can you enable buckets?
ans: set hive.enforce.bucketing = true;

27.How does bucketing help in the faster execution of queries?
ans: As the shuffling and sorting job is done prior to any other queries , which helps in optimizing joins.

28.How to optimise Hive Performance? Explain in very detail.
ans: 
1) Use suitable joins when required : bucket map join, brodscast join, sort merge join
2)Use hive cost based optimizer
3)Use TEZ, allows unneccessary access to disk. set.hive.execution.engine = TEZ
4) select columns only which are required.

29. What is the use of Hcatalog?
ans:Hcatalog is a tool used to access hive metastore tables using pig,spark sql and other mapreduce apps.

30. Explain about the different types of join in Hive. 
ans: Brodcast join - every reducer has one of the tables entire copy which joins with the other table, this isasn done only when one of the table is small.
No reducers need to be used in this as no shuffling.

Bucket map join - Used when both the tables have buckets from the same column and join is done on those buckets
Used when there are large tables
Used when the buckets in both tables are multiples of each other.
Sort merge join -As the name suggests it sorts teh data first in the reducers and then merges the data.

31.Is it possible to create a Cartesian join between 2 tables, using Hive?
ans: Using join and putting the condition in where clause miogh help.

32.Explain the SMB Join in Hive?
ans: Sort merge bucket  join - As the name suggests it sorts teh data first in the reducers and then merges the data.

33.What is the difference between order by and sort by which one we should  use?
ans: Order by in hive is the ordering at the global level of reducers
whereas sort by is sorting loaclly in a reducer.
To get complete ordering of data use order by.

34.What is the usefulness of the DISTRIBUTED BY clause in Hive?
ans: Distributed by is used Hive to distribute data to reducers according to the column mentioned.

35.How does data transfer happen from HDFS to Hive? 
ans: data for tables is stored in hdfs or hive's data warehouse directory.
If you have data in hdfs then create a table in hive with the schema in orc format then you will be able to read.

36.Wherever (Different Directory) I run the hive query, it creates a new  metastore_db, please explain the reason for it?
ans: Hive checks first that whether the metastore db is created or not and by default the property is set to create it.
You will have to change that in the configuration file , the property being:
javax.jdo.option.ConnectionURL
with the default value:
jdbc:derby:;databaseName=metastore_db;create=true

37.What will happen in case you have not issued the command: ‘SET 
ans: hive.enforce.bucketing=true;’ before bucketing a table in Hive?
If this is not done the number of files in the table directory will not be equal to the number of buckets.

38.Can a table be renamed in Hive?
ans: Yes, by using the ALTER command.

39.Write a query to insert a new column(new_col INT) into a hive table at a position before an existing column (x_col)
ans:
Alter table tablename
change column new_col INT
before old_col;

40.What is serde operation in HIVE?
ans: Serde- Serialization and deserilization.
Serde allows reading of data in hive , in a table and writing it back to hdfs.

41.Explain how Hive Deserializes and serialises the data?
Serilization - converting youribject/ data into bytes
Deserilization - converting your data in bytes to object
Rowformat syntax - describes the libraries for conversion.
Stored as- tells the input and output format for mapreduce.

42.Write the name of the built-in serde in hive ?
ans:hive uses a SerDe called LazySimpleSerDe

43.What is the need of custom Serde?
ans: A SerDe allows hive to read the data from the table and write it back to the HDFS in any custom format.

44.Can you write the name of a complex data type(collection data types) in Hive?
ans:Hive supports 3 types of Complex Data Types STRUCT, MAP and ARRAY. They are also know as collection or nested datatypes. They can store multiple values in a single row/column.

45.Can hive queries be executed from script files? How?
ans:Hive -f command The Hive -f command is used to execute one or more hive queries from a file in batch mode.Instead of enter into the Hive CLI and execute the queries one by one,We can directly execute the set of queries using Hive -f option from the command line itself. Syntax of Hive -f command


46.What are the default record and field delimiter used for hive text files?
ans:The default record delimiter is n, and the filed delimiters are 001, 002, and 003.

47.How do you list all databases in Hive whose name starts with s?
ans:SHOW DATABASES LIKE 's*';

48.What is the difference between LIKE and RLIKE operators in Hive?
ans:The difference between Like and Rlike in Hive tags: hive You can use Like and Rlike to make a blur match, which uses SQL wildcards, while rlike uses regular matching.

49.How to change the column data type in Hive?
ans: ALTER TABLE table_name [PARTITION partition_spec] CHANGE [COLUMN] col_old_name col_new_name column_type
eg:

CREATE TABLE test_change (a int, b int, c int);
 
// First change column a's name to a1.
ALTER TABLE test_change CHANGE a a1 INT

50.How will you convert the string ’51.2’ to a float value in the particular column?
ans:we can use cast(column_name as float)

51.What will be the result when you cast ‘abc’ (string) as INT in hive ?
ans: If you cast ‘abc’ (string) as INT in Hive, you will get a conversion error. This is because ‘abc’ is not a valid integer value and cannot be converted to one.

52.What does the following query do?
ans:
It will overwrite any existing data in the table employees or its partitions1.
It will create partitions based on the columns country and state in the table employees1.
It will select all the columns from the table staged_employees and also the columns cnty and st as aliases for country and state respectively2.
It will insert the selected data into the table employees according to the partitions

53.Write a query where you can overwrite data in a new table from the existing table.
ans:INSERT OVERWRITE TABLE new_table SELECT * FROM existing_table;

54.What is the maximum size of a string data type supported by Hive? Explain how Hive supports binary formats.
ans:The maximum size of a string data type supported by Hive is 2 GB12. This means that a string column can store up to 2 GB of characters in Hive.
Hive supports binary formats by using the binary data type, which can store any binary data such as images or encrypted data3. The binary data type has no predefined length or range. To use the binary data type, you need to specify it in the table schema, for example:
CREATE TABLE my_table (id int, image binary);
You also need to use a suitable SerDe (Serializer/Deserializer) to read and write binary data in Hive. For example, you can use the org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe for sorting and comparing binary data.

55. What File Formats and Applications Does Hive Support?
ans:
* TextFile: This is the default file format in Hive, which stores data as plain text files with a delimiter (such as comma or tab) to separate fields. TextFile is easy to read and write, but it takes more space and time to process123.

* SequenceFile: This is a binary file format that stores data as key-value pairs. SequenceFile is more compact and efficient than TextFile, but it requires a SerDe (Serializer/Deserializer) to read and write data124.

* RCFile (Record Columnar File): This is a columnar file format that stores data in a table-like structure, where each column is stored separately. RCFile is more suitable for analytical queries that access only a few columns, as it reduces the I/O cost and improves the compression ratio124.

* ORC (Optimized Row Columnar): This is an advanced columnar file format that supports various features such as compression, indexing, encryption, statistics, etc. ORC improves the performance and storage efficiency of Hive queries by reducing the data size and enabling faster processing124.

* Avro: This is a row-based file format that stores data with a schema definition. Avro supports schema evolution, which means that the schema can be changed without breaking the compatibility with existing data. Avro also supports complex data types such as arrays, maps, unions, etc24.

* Parquet: This is another columnar file format that stores data in a nested structure. Parquet is designed for efficient query processing on large-scale data sets. Parquet also supports complex data types and schema evolution24.
  Hive also supports custom file formats by using custom INPUTFORMAT and OUTPUTFORMAT classes2.

Hive can be used for various applications such as data warehousing, data analysis, data mining, reporting, etc. Hive can also integrate with other Hadoop components such as Pig, MapReduce, Spark, etc., to perform complex data processing tasks.

56.How do ORC format tables help Hive to enhance its performance?
ans:ORC help to enhance the performance many ways
reduceing data size
indexing data
skipping unneccessary column
vectorization these all helps to increase the performance of ORC file format.

57.How can Hive avoid mapreduce while processing the query?
ans: Hive can avoid mapreduce while processsing in some cases those are given below
simple queries that means queries that doesn't having aggregation,filter,join....
if query execute in local mode it doesnot need mapreduce it uses local resources and query optimization will help to avoid mapreduce.

58.What is view and indexing in hive?
ans: A view in Hive is a logical representation of a query result set. It is not a physical table that stores data, but a virtual table that can be queried like a regular table.
An index in Hive is a pointer to a specific column of a table that can improve the query performance by reducing the amount of data to be scanned. An index table is different from the main table and stores the values of the indexed column and the corresponding row IDs. An index can be created using the CREATE INDEX statement and can be used by Hive automatically or manually.


59.Can the name of a view be the same as the name of a hive table?
ans: No, the name of a view cannot be the same as the name of a hive table. The name of a view must be unique and it cannot be the same as any table or database or view’s name. If you try to create a view with the same name as an existing table, you will get an error message saying that the table already exists.

60.What types of costs are associated in creating indexes on hive tables?
ans:Mainly two cost are there associatedwith creating indexing in hive table cost of storage and processing1ng
Processing cost refers to the additional time and resources required to create and maintain the index table.
Storage cost refers to the additional disk space required to store the index table.
These costs need to be weighed against the benefits of using indexes, such as faster query performance and reduced data scanning. Depending on the use case and query pattern, indexes may or may not be worth creating.

61.Give the command to see the indexes on a table?
ans:We can use show indexes or formatted key word also we can use to show the indexes. 
eg: SHOW INDEXES ON emp;

62. Explain the process to access subdirectories recursively in Hive queries.
ans: The process to access subdirectories recursively in Hive queries is to set two configuration properties to true: mapred.input.dir.recursive and hive.mapred.supports.subdirectories
These properties enable Hive to scan all the subdirectories of a given directory and use them as input for MapReduce jobs. By default, these properties are set to false, which means Hive will only access the files in the top-level directory4.

To set these properties, you can use the following commands in Hive:

SET mapred.input.dir.recursive=true;
SET hive.mapred.supports.subdirectories=true;


63.If you run a select * query in Hive, why doesn't it run MapReduce?
ans:If you run a select * query in Hive, it doesn’t run MapReduce because it is just dumping the data from HDFS without doing any aggregation or transformation.

64.What are the uses of Hive Explode?
ans:Hive Explode is a function that takes an array or a map as an input and outputs the elements as separate rows1. It is used with LATERAL VIEW to expand the array or map into rows.


65. What is the available mechanism for connecting applications when we run Hive as a server ?
ans : HiveServer2 is based on Apache Thrift, which is a framework for cross-language services development. Thrift allows clients to communicate with Hive using different protocols, such as binary, HTTP, or SAS.

66.Can the default location of a managed table be changed in Hive?
ans:Two ways we can do that using location
eg: CREATE TABLE demo (id int, name string) LOCATION 'hdfs://localhost:9000/custom_path';
using alter database
eg:ALTER DATABASE student SET LOCATION 'hdfs://localhost:9000/hive_db';


67.What is the Hive ObjectInspector function?
ans:the Object-Inspector helps to analyze the internal structure of a row object and individual structure of columns.
ObjectInspector is used by Hive for various purposes, such as:

Serializing and deserializing data between different formats (such as Thrift, Java, Avro, etc.)23.
Performing type conversions and validations23.
Implementing user-defined functions (UDFs), aggregations (UDAFs), and table functions (UDTFs)

68.What is UDF in Hive?
ans: it's user defind function in hive It is a way to create your own custom functions in Hive that are not available in the built-in functions.

69.Write a query to extract data from hdfs to hive.
ans:
Using LOAD DATA INPATH statement: This method moves the data file from HDFS to Hive’s default warehouse directory. It does not copy or modify the data file. The data file must match the schema and format of the Hive table.
Using CREATE EXTERNAL TABLE statement: This method creates a Hive table that points to an existing data file in HDFS. It does not move or copy the data file. The data file must match the schema and format of the Hive table.
Using INSERT OVERWRITE DIRECTORY statement: This method copies and converts the data from a Hive table to a specified directory in HDFS. It allows you to specify the output format and delimiter of the data file.

70.What is TextInputFormat and SequenceFileInputFormat in hive?
ans:TextInputFormat reads data in plain text file format. It treats each line as a record and splits the file by newline characters. It uses HiveIgnoreKeyTextOutputFormat as the corresponding OutputFormat class.
SequenceFileInputFormat reads data in Hadoop SequenceFile format. It is a binary file format that stores key-value pairs. It uses SequenceFileOutputFormat as the corresponding OutputFormat class.

71.How can you prevent a large job from running for a long time in a hive?
ans:
Setting hive.mapred.mode to strict: This mode restricts queries on partitioned tables to have a WHERE clause that filters on at least one partition column. This prevents scanning all the partitions and reduces the amount of data processed.
Tuning your Hive query design: You can optimize your queries by using appropriate join types, filtering columns and rows, avoiding subqueries and UDFs, using indexes and statistics, etc. These techniques can improve the performance and efficiency of your queries.
Adjusting your cluster configuration: You can increase the size and number of containers allocated for your Hive jobs, choose a suitable execution engine (such as Tez or Spark), use a larger cluster coordinator node, etc. These factors can affect the speed and parallelism of your jobs.
Killing or cancelling your job: If you want to stop a running job manually, you can use the hadoop job -kill <job_id> or mapred job -kill <job_id> command. You can find the job ID from the log traces or the web UI of your cluster.

72..What are the uses of Hive Explode?
ans:Hive Explode is a function that takes an array or a map as an input and outputs the elements as separate rows1. It is used with LATERAL VIEW to expand the array or map into rows.

73.Can Hive process any type of data formats? Why? Explain in very details ?
ans:Yes, Hive can process different types of data formats. Hive supports four file formats: TEXTFILE, SEQUENCEFILE, ORC and RCFILE (Record Columnar File)1. Hive also supports most of the primitive data types that we find in relational databases and three collection data types that are rarely supported by relational databases2. Hive is schema on read, which means that before storing the data, no validation is required, and you can dump the data. While querying/fetching the data, proper structure is given to the data.

74.Whenever we run a Hive query, a new metastore_db is created. Why?
ans:Whenever we run a Hive query, a new metastore_db is created because the location of the metastore (metastore_db) is a relative path. Therefore, it gets created where you launch Hive.

75.Can we change the data type of a column in a hive table? Write a complete query.
ans: Yes, we can change the data type of a column in a hive table using the ALTER TABLE command. The syntax for changing the data type of a column in a hive table is as follows: ALTER TABLE table_name CHANGE old_col_name new_col_name new_data_type.

76.While loading data into a hive table using the LOAD DATA clause, how do you specify it is a hdfs file and not a local file ?
ans: To specify that the file is in HDFS and not a local file while loading data into a hive table using the LOAD DATA clause, you do not need to specify anything. By default, Hive considers the specified path as an HDFS location1. If you want to load a file from the local filesystem into the Hive table without uploading to HDFS, you can use the LOCAL.

77.What is the precedence order in Hive configuration?
ans: In Hive, the order of precedence of the config files is as follows (later one has higher precedence): hive-site.xml -> hivemetastore-site.xml -> hiveserver2-site.xml -> -hiveconf commandline parameters1. The SET command in Hive has the highest priority.

78.Which interface is used for accessing the Hive metastore ?
ans: The interface used for accessing the Hive metastore is the WebHCat API1. With WebHCat, applications can make HTTP requests to access the Hive metastore (HCatalog DDL) or to create and queue Hive queries and commands, Pig jobs, and MapReduce or YARN jobs (either standard or streaming).

79.Is it possible to compress json in the Hive external table ?
ans: Yes, it is possible to compress JSON in the Hive external table. You can gzip your files and put them as is (*.gz) into the table location.

80.What is the difference between local and remote metastores?
ans:In local metastore configuration, the metastore service runs in the same JVM in which the Hive service is running and connects to a database running in a separate JVM, either on the same machine or on a remote machine. In the remote metastore configuration, the metastore service runs on its own separate JVM and not in the Hive service JVM. The main advantage of remote mode over local mode is that remote mode does not require the administrator to share JDBC login information for the metastore database with each Hive user.

81.What is the purpose of archiving tables in Hive?
ans:Archiving tables in Hive is used to reduce the number of files in partitions. The main advantage of archiving is that it will decrease the number of files to be stored in NameNode, which makes it easier for the NameNode to manage1. Hive has built-in support for converting files on existing partitions to a Hadoop Archive. We can even query an archived partition in Hive.

82.What is DBPROPERTY in Hive?
ans:DBPROPERTY is a command in Hive that allows you to set or modify properties for a database. The properties are key-value pairs that provide additional information about the database. You can use the DBPROPERTY command with the ALTER DATABASE command to set or modify the properties. Here is an example of how to use the DBPROPERTY command:
ALTER DATABASE mydatabase SET DBPROPERTIES ('mykey'='myvalue');

83.Differentiate between local mode and MapReduce mode in Hive ?
ans:In local mode, MapReduce jobs related to Hive run locally on a user machine. This is the default mode in which Hadoop uses the local file system. In MapReduce mode, Hive as well as Hadoop is running in a fully distributed mode. The major difference between the two modes is that in local mode, the data is stored on the local file system, while in MapReduce mode, the data is stored in the Hadoop Distributed File System (HDFS). Local mode is useful for developing and testing Hive queries on small data sets, while MapReduce mode is used for processing large data sets in a distributed environment.
